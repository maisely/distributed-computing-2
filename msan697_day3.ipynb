{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes:\n",
    "#### Overall Idea:\n",
    "+ Very good baseline for classification - It's always good to have a baseline model to compare against\n",
    "+ General, can be applied to different problems\n",
    "+ Can be used with ngrams - very useful\n",
    "\n",
    "#### Posterior Class Probability\n",
    "- For a document *d* and a class *c*, where MAP is **\"max a posteriori\"** or most likely class: \n",
    "\n",
    "$c_{MAP} = argmax \\ P(c \\ | \\ d) \n",
    "         = argmax \\ P(\\ d \\ | \\ c )P(c) / P(d)$ \n",
    "\n",
    "- i.e. $P(c \\ | \\ d)$ depends on the **training data** or the choice of the **modelling technique**\n",
    "\n",
    "#### Independence Assumptions \n",
    "- Assume the position of the word does not matter (bag of words)\n",
    "- Assume the features $x_i \\ |\\ c$ are independent for every class $c$ where $x_i$ is the occurence of a word\n",
    "- That means $P(x_1, x_2, ..., x_n \\ | \\ c) = P(x_1 \\ | \\ c) P(x_2 \\ | \\ c) ... P(x_n \\ | \\ c) $\n",
    "\n",
    "#### Multinomial Naive Bayes Classifier\n",
    "\n",
    "- $c_{MAP} = argmax \\ P(x_1, x_2, ..., x_n \\ | \\ c)P(c)$\n",
    "- $c_{NB} = argmax \\ P(c_j) \\ | \\ P(x \\ | \\ c)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaÃ¯ve Bayes Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Missing vocabulary in the training data -> End up giving a zero score to the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace (add-1) Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **For every class, compute priors**\n",
    "\n",
    "$\\hat P (c) = \\frac{N_c}{N}$ where $N$ = # of docs and $N_c$ is # of documents in class c\n",
    "\n",
    "- **For every word and every class**:\n",
    "\n",
    "$\\hat P(w|c) = \\frac {count(w, c) + 1}{count(c) + |V| + 1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "path = \"./data/\"\n",
    "train_path = path + \"aclImdb/train/\"\n",
    "test_path = path + \"aclImdb/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_raw_pos = sc.textFile(train_path + \"pos/*.txt\")\n",
    "data_raw_neg = sc.textFile(train_path + \"neg/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw_pos.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample 20% of the data\n",
    "data_raw_pos = data_raw_pos.sample(False, 0.2, 1)\n",
    "data_raw_neg = data_raw_neg.sample(False, 0.2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of partitions\n",
    "data_raw_pos.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You may OR may NOT want to repartition or coalesce\n",
    "# num_partitions = 3 or 4 times the number of CPUs\n",
    "num_partitions = 8\n",
    "data_raw_pos = data_raw_pos.repartition(num_partitions)\n",
    "data_raw_neg = data_raw_neg.repartition(num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2529\n",
      "2529\n"
     ]
    }
   ],
   "source": [
    "# count 2529 elements\n",
    "# this takes some time\n",
    "print(data_raw_pos.count())\n",
    "print(data_raw_neg.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A',\n",
       " u'lot',\n",
       " u'of',\n",
       " u'people',\n",
       " u'are',\n",
       " u'saying',\n",
       " u'that',\n",
       " u'Al',\n",
       " u'Pacino',\n",
       " u'over']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into words (here we could filter stepwords, clean, rm punctuation)\n",
    "data_pos = data_raw_pos.flatMap(lambda x: x.split())\n",
    "data_pos.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'A', 1),\n",
       " (u'lot', 1),\n",
       " (u'of', 1),\n",
       " (u'people', 1),\n",
       " (u'are', 1),\n",
       " (u'saying', 1),\n",
       " (u'that', 1),\n",
       " (u'Al', 1),\n",
       " (u'Pacino', 1),\n",
       " (u'over', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform to value pairs to be able to count\n",
    "data_pos = data_pos.map(lambda x: (x, 1))\n",
    "data_pos.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'remastered', 1),\n",
       " (u'gangs.', 1),\n",
       " (u'anyways.I', 1),\n",
       " (u'wonderfull', 1),\n",
       " (u'four', 68),\n",
       " (u'Francisco,', 4),\n",
       " (u'demotes', 1),\n",
       " (u'HE-MAN', 1),\n",
       " (u'(sometimes', 2),\n",
       " (u'electricity', 5)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting number of words\n",
    "data_pos = data_pos.reduceByKey(lambda x,y:x+y)\n",
    "data_pos.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Jojo', 2),\n",
       " (u'/>of', 3),\n",
       " (u\"syberberg's\", 1),\n",
       " (u'lungs.<br', 1),\n",
       " (u'Montford.', 1),\n",
       " (u'increase', 2),\n",
       " (u'jobbing', 1),\n",
       " (u'electricity', 1),\n",
       " (u\"'Breakfast\", 3),\n",
       " (u're-united', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can do all together\n",
    "data_neg = data_raw_neg.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\n",
    "data_neg.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute count(pos) and count(neg) or how many words in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_pos = data_pos.map(lambda x: x[1]).reduce(lambda x,y:x+y)\n",
    "count_neg = data_neg.map(lambda x: x[1]).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(604442, 572063)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pos, count_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute V or number of unique words in all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101820\n"
     ]
    }
   ],
   "source": [
    "## Let's get V\n",
    "v1 = data_pos.map(lambda x: x[0]) # pos vocabulary\n",
    "v2 = data_neg.map(lambda x: x[0]) # neg vocabulary\n",
    "v = v1.union(v2)\n",
    "\n",
    "V = v.distinct().count()\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the P(w|c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that the denominators are different \n",
    "pos_denom = float(count_pos + V + 1)\n",
    "neg_denom = float(count_neg + V + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log probabities\n",
    "pos_prob = data_pos.map(lambda x: (x[0], np.log(float(x[1] + 1)/pos_denom)))\n",
    "neg_prob = data_neg.map(lambda x: (x[0], np.log(float(x[1] + 1)/neg_denom))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'remastered', -12.77459578779308),\n",
       " (u'gangs.', -12.77459578779308),\n",
       " (u'anyways.I', -12.77459578779308),\n",
       " (u'wonderfull', -12.77459578779308),\n",
       " (u'four', -9.2336364637557669)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_prob.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to dictionaries\n",
    "pos_prob = dict(pos_prob.collect())\n",
    "neg_prob = dict(neg_prob.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# broadcast = shared by all nodes\n",
    "pos_prob_b = sc.broadcast(pos_prob)\n",
    "neg_prob_b = sc.broadcast(neg_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2529\n",
      "2529\n"
     ]
    }
   ],
   "source": [
    "# convert test set \n",
    "test_raw_pos = sc.textFile(test_path + \"pos/*.txt\")\n",
    "test_raw_neg = sc.textFile(test_path + \"neg/*.txt\")\n",
    "\n",
    "test_raw_pos = test_raw_pos.sample(False, 0.2, 1)\n",
    "test_raw_neg = test_raw_neg.sample(False, 0.2, 1)\n",
    "\n",
    "num_partitions = 8\n",
    "test_raw_pos = test_raw_pos.repartition(num_partitions)\n",
    "test_raw_neg = test_raw_neg.repartition(num_partitions)\n",
    "\n",
    "print(test_raw_pos.count())\n",
    "print(test_raw_neg.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Why this film was only released in 4 states is beyond me. I thought this film was a divine story. The name says it all: Seeing Other People. This movie has more logic than laughs, which I suppose is why it works so well. Common sense also makes an appearance in what would seem to be another puerile sex comedy. Alice is getting her feet frozen in the cold, when she feels irrationally about the way she might perform for her fianc\\xe9, not just sexually, but as a partner, and friend etc. This starts what seems to be an almost archetypal journey for the both of them. One fling after another leads to trouble, as if it wasn't a bad idea from the start. Witty dialogue and comic set-ups make this one funny as hell! Nicholson and Mohr set the tone of the film early on, and keep the promise they anticipate. Other highlights are Lauren Graham, Andy Richter, and Helen Slater(in her first theatrical film in 10 years!). Climax begins to take an insane turn, but a simple ending makes this one far more enjoyable than most movies today. Mohr fans will see something different in his Ed character, and fans of Helen Slater will enjoy her shiny moments of a quick, but excellent come-back. Any Richter takes home the award for most moralistic character. Romantic and funny, or just plain fun. Seeing Other People is a gem, which needs to be noticed.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_pos.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_class(doc):\n",
    "    words = doc.split(\" \")\n",
    "    counts = Counter(words) # frequency of words in the document\n",
    "    log_pos = 0.0\n",
    "    log_neg = 0.0\n",
    "    for w in counts:\n",
    "        log_pos += counts[w]* pos_prob_b.value.get(w, np.log(1.0/pos_denom))\n",
    "        log_neg += counts[w]* neg_prob_b.value.get(w, np.log(1.0/neg_denom))\n",
    "    if log_pos > log_neg:\n",
    "        return \"pos\"\n",
    "    return \"neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = test_raw_pos.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_class(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'neg', 'neg', 'pos', 'pos']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pos_res = test_raw_pos.map(pred_class)\n",
    "test_pos_res.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 575, 'pos': 1954}\n"
     ]
    }
   ],
   "source": [
    "test_pos_res = test_raw_pos.map(pred_class).map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\n",
    "pos_results = dict(test_pos_res.collect())\n",
    "print(pos_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 2153, 'pos': 376}\n"
     ]
    }
   ],
   "source": [
    "test_neg_res = test_raw_neg.map(pred_class).map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\n",
    "neg_results = dict(test_neg_res.collect())\n",
    "print(neg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811981020166\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "total = sum(neg_results.values()) + sum(pos_results.values())\n",
    "acc = float(neg_results[\"neg\"] + pos_results[\"pos\"]) / float(total)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise\n",
    "\n",
    "1. Improve data cleaning\n",
    "2. How to run for all data\n",
    "3. Top $K$ bi-grams\n",
    "   + Compute bi-gram frequency and select top bi-gram (broadcast bigrams)\n",
    "   + Concatenate bigrams \"San Francisco\" ==> \"San_Francisco\"\n",
    "   + Re-run Naiive Bayes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
