{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Saving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local File System\n",
    "\n",
    "- Specify \n",
    "```\n",
    "sc.textFile(file:///path)\n",
    "```\n",
    "\n",
    "- The filesystem should be available at the same path on all nodes in the cluster for both the master and executors\n",
    "- If the file is not on all nodes in the cluster, load it locally and then call parallelize to distribute the contents to workers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S3 (Simple Storage Service) – Web Storage\tService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = sc.textFile('s3n://usfca-msan694/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3n://usfca-msan694/README.md MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " u'supports general computation graphs for data analysis. It also supports a']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt.collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HDFS \n",
    "\n",
    "- Designed to work on commodity hardware and be resilient to node failure, while providing high data throughput\n",
    "- Specify \"hdfs://master:port/path\" for input and output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "\n",
    "- Handle structured, distributed data with a table-like representation with named column declared with column types\n",
    "- `RDD`: low-level and direct way of manipulating data in Spark. Do not have to be structured \n",
    "- Special case of DataSet type\n",
    "- Can join, query and save DataFrames\n",
    "- SparkSQL lets you register DataFrames from different sources (SQL, Parquet, JSON, ORC) as tables in the table catalog and query them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrames\n",
    "\n",
    "- **Convert existing RDDs** - Load data as text, parse the lines and identify elements\n",
    "    1. Use RDDs containing row data as tuples \n",
    "    2. Specify scheme using `createDataFrame`\n",
    "        - Use `createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Use RDDs containing row data as tuples \n",
    "- cannot speicify all schema attributes (e.g. nullable or not)\n",
    "- convert the RDD element arrays to tuples and call **`toDF`**  on the resulting RDD\n",
    "- can specify column names when calling the `toDF` method\n",
    "- all the columns are string type and nullable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "- Load the italianPosts.csv parsed with ”~” into an RDD.\n",
    "- Create DataFrame, using RDDs containing row data as tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1. Load the italianPosts.csv parsed with ”~” into an RDD.\n",
    "# data = sc.textFile('s3n://usfca-msan694/Italian_Stack_Exchange/italianPosts.csv')\n",
    "data = sc.textFile('./data/Italian_Stack_Exchange/italianPosts.csv')\n",
    "data = data.map(lambda x: x.split(\"~\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2. Create DataFrame, using RDDs containing row data as tuples:\n",
    "dataRDD = data.map(lambda x: tuple([x[i] for i in range(len(x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string, _11: string, _12: string, _13: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HAVE TO SHUT DOWN ALL OTHER NOTEBOOOKS THAT ARE RUNNING\n",
    "dataRDD.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---+--------------------+---+--------------------+----+--------------------+-------------------+----+----+---+----+\n",
      "| _1|                  _2| _3|                  _4| _5|                  _6|  _7|                  _8|                 _9| _10| _11|_12| _13|\n",
      "+---+--------------------+---+--------------------+---+--------------------+----+--------------------+-------------------+----+----+---+----+\n",
      "|  4|2013-11-11 18:21:...| 17|&lt;p&gt;The infi...| 23|2013-11-10 19:37:...|null|                    |                   |null|null|  2|1165|\n",
      "|  5|2013-11-10 20:31:...| 12|&lt;p&gt;Come cre...|  1|2013-11-10 19:44:...|  61|Cosa sapreste dir...|&lt;word-choice&gt;|   1|null|  1|1166|\n",
      "|  2|2013-11-10 20:31:...| 17|&lt;p&gt;Il verbo...|  5|2013-11-10 19:58:...|null|                    |                   |null|null|  2|1167|\n",
      "+---+--------------------+---+--------------------+---+--------------------+----+--------------------+-------------------+----+----+---+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataRDD.toDF().show(n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+--------------------+-------------------+-----------+----------------+----------+----+\n",
      "|commentCount|    lastActivityDate|ownerUserId|                body|score|        creationDate|viewCount|               title|               tags|answerCount|acceptedAnswerId|postTypeId|  id|\n",
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+--------------------+-------------------+-----------+----------------+----------+----+\n",
      "|           4|2013-11-11 18:21:...|         17|&lt;p&gt;The infi...|   23|2013-11-10 19:37:...|     null|                    |                   |       null|            null|         2|1165|\n",
      "|           5|2013-11-10 20:31:...|         12|&lt;p&gt;Come cre...|    1|2013-11-10 19:44:...|       61|Cosa sapreste dir...|&lt;word-choice&gt;|          1|            null|         1|1166|\n",
      "|           2|2013-11-10 20:31:...|         17|&lt;p&gt;Il verbo...|    5|2013-11-10 19:58:...|     null|                    |                   |       null|            null|         2|1167|\n",
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+--------------------+-------------------+-----------+----------------+----------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDf = dataRDD.toDF([\"commentCount\", \"lastActivityDate\", \"ownerUserId\", \n",
    "     \"body\", \"score\", \"creationDate\", \"viewCount\", \"title\", \"tags\", \n",
    "     \"answerCount\", \"acceptedAnswerId\", \"postTypeId\", \"id\"])\n",
    "\n",
    "itPostDf.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- commentCount: string (nullable = true)\n",
      " |-- lastActivityDate: string (nullable = true)\n",
      " |-- ownerUserId: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- creationDate: string (nullable = true)\n",
      " |-- viewCount: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- answerCount: string (nullable = true)\n",
      " |-- acceptedAnswerId: string (nullable = true)\n",
      " |-- postTypeId: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Specify scheme using `createDataFrame`\n",
    "\n",
    "- Format: \n",
    "```python \n",
    "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)\n",
    "```\n",
    "    + `data`: an RDD of row/tuple/list/dict, list or pd.DataFrame. e.g. \n",
    "    ```python\n",
    "    from pyspark.sql import Row\n",
    "    Row(val1, val2, val3)\n",
    "    ```\n",
    "    + `schema`: a `StructType` or list of column names, cannot be None, **REQUIRED**\n",
    "    + `samplingRatio`: sample ratio of rows used for inferring the schema\n",
    "    + `verifySchema`: verify data type of very row against schema\n",
    "\n",
    "\n",
    "- **`schema`**\n",
    "\n",
    "    - Example\n",
    "    ```python\n",
    "    from pyspark.sql.types import *\n",
    "    schema = StructType([StructField(\"name\",  StringType(), True), \n",
    "                        StructField(\"age\", IntegerType(), True)])\n",
    "    ```\n",
    "\n",
    "    - StructType : consisting of a list of StructField\n",
    "    - StructField: including \n",
    "        + **column name** (string), \n",
    "        + data type\n",
    "        + nullable (default: True), \n",
    "        + metadata(default: None)\n",
    "    - data type: `NullType(), StringType(), BinaryType(), BooleanType(), DateType(), TimestampType(), DoubleType(), FloatType(), IntegerType()`, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 \n",
    "Create DataFrame, specifying a schema using `createDataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1. Load the italianPosts.csv parsed with ”~” into an RDD.\n",
    "# data = sc.textFile('s3n://usfca-msan694/Italian_Stack_Exchange/italianPosts.csv')\n",
    "data = sc.textFile('./data/Italian_Stack_Exchange/italianPosts.csv')\n",
    "# data = data.map(lambda x: x.split(\"~\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toIntSafe(inval):\n",
    "  try: return int(inval)\n",
    "  except ValueError: return None\n",
    "\n",
    "def toTimeSafe(inval):\n",
    "  try: return datetime.strptime(inval, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "  except ValueError: return None\n",
    "\n",
    "def toLongSafe(inval):\n",
    "  try: return long(inval)\n",
    "  except ValueError: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stringToPost(row):\n",
    "  r = row.encode('utf8').split(\"~\")\n",
    "  return Row(\n",
    "    toIntSafe(r[0]),\n",
    "    toTimeSafe(r[1]),\n",
    "    toIntSafe(r[2]),\n",
    "    r[3],\n",
    "    toIntSafe(r[4]),\n",
    "    toTimeSafe(r[5]),\n",
    "    toIntSafe(r[6]),\n",
    "    toIntSafe(r[7]),\n",
    "    r[8],\n",
    "    toIntSafe(r[9]),\n",
    "    toLongSafe(r[10]),\n",
    "    toLongSafe(r[11]),\n",
    "    long(r[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Define a StructType - list of columns\n",
    "postSchema = StructType([\n",
    "    StructField(\"commentCount\", IntegerType(), True), \n",
    "    StructField(\"lastActivityDate\", TimestampType(), True), \n",
    "    StructField(\"ownerUserId\", LongType(), True),\n",
    "    StructField(\"body\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"creationDate\", TimestampType(), True),\n",
    "    StructField(\"viewCount\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"tags\", StringType(), True),\n",
    "    StructField(\"answerCount\", IntegerType(), True),\n",
    "    StructField(\"acceptedAnswerId\", LongType(), True),\n",
    "    StructField(\"postTypeId\", LongType(), True),\n",
    "    StructField(\"id\", LongType(), False) # id cannot be null\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Convert input data\n",
    "rowRDD = data.map(lambda x: stringToPost(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5: Create DataFrame with Schema\n",
    "sqlContext = SQLContext(sc)\n",
    "itPostDF = sqlContext.createDataFrame(rowRDD, postSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+-----+-------------------+-----------+----------------+----------+----+\n",
      "|commentCount|    lastActivityDate|ownerUserId|                body|score|        creationDate|viewCount|title|               tags|answerCount|acceptedAnswerId|postTypeId|  id|\n",
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+-----+-------------------+-----------+----------------+----------+----+\n",
      "|           4|2013-11-11 18:21:...|         17|&lt;p&gt;The infi...|   23|2013-11-10 19:37:...|     null| null|                   |       null|            null|         2|1165|\n",
      "|           5|2013-11-10 20:31:...|         12|&lt;p&gt;Come cre...|    1|2013-11-10 19:44:...|       61| null|&lt;word-choice&gt;|          1|            null|         1|1166|\n",
      "|           2|2013-11-10 20:31:...|         17|&lt;p&gt;Il verbo...|    5|2013-11-10 19:58:...|     null| null|                   |       null|            null|         2|1167|\n",
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+-----+-------------------+-----------+----------------+----------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDF.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- commentCount: integer (nullable = true)\n",
      " |-- lastActivityDate: timestamp (nullable = true)\n",
      " |-- ownerUserId: long (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- creationDate: timestamp (nullable = true)\n",
      " |-- viewCount: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- answerCount: integer (nullable = true)\n",
      " |-- acceptedAnswerId: long (nullable = true)\n",
      " |-- postTypeId: long (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame API Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic APIs\n",
    "- `select(<column_names>)` or `select(<list of column_names>)`\n",
    "- `drop()`\n",
    "- `filter()`, `where()`\n",
    "- `withColumnRenamed(old_name, new_name)`\n",
    "- `withColumn()` - rename and add columns\n",
    "- `orderBy()`, `sort()`\n",
    "- Check out: \thttps://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|                body|\n",
      "+----+--------------------+\n",
      "|1165|&lt;p&gt;The infi...|\n",
      "|1166|&lt;p&gt;Come cre...|\n",
      "|1167|&lt;p&gt;Il verbo...|\n",
      "|1168|&lt;p&gt;As part ...|\n",
      "|1169|&lt;p&gt;&lt;em&g...|\n",
      "|1170|&lt;p&gt;There's ...|\n",
      "|1171|&lt;p&gt;As other...|\n",
      "|1172|&lt;p&gt;The expr...|\n",
      "|1173|&lt;p&gt;When I w...|\n",
      "|1174|&lt;p&gt;Wow, wha...|\n",
      "+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select()\n",
    "itPostDF.select(\"id\", \"body\").show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+-----+---------+-----+-----------+----+\n",
      "|commentCount|ownerUserId|                body|score|viewCount|title|answerCount|  id|\n",
      "+------------+-----------+--------------------+-----+---------+-----+-----------+----+\n",
      "|           4|         17|&lt;p&gt;The infi...|   23|     null| null|       null|1165|\n",
      "|           5|         12|&lt;p&gt;Come cre...|    1|       61| null|          1|1166|\n",
      "|           2|         17|&lt;p&gt;Il verbo...|    5|     null| null|       null|1167|\n",
      "|           1|        154|&lt;p&gt;As part ...|   11|      187| null|          4|1168|\n",
      "|           0|         70|&lt;p&gt;&lt;em&g...|    3|     null| null|       null|1169|\n",
      "|           2|         17|&lt;p&gt;There's ...|    8|     null| null|       null|1170|\n",
      "|           1|         63|&lt;p&gt;As other...|    3|     null| null|       null|1171|\n",
      "|           1|         63|&lt;p&gt;The expr...|    1|     null| null|       null|1172|\n",
      "|           9|         63|&lt;p&gt;When I w...|    5|      122| null|          3|1173|\n",
      "|           0|         18|&lt;p&gt;Wow, wha...|    5|     null| null|       null|1174|\n",
      "+------------+-----------+--------------------+-----+---------+-----+-----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop() - select all but one/some\n",
    "itPostDF.drop(\"lastActivityDate\", \"creationDate\", \"tags\", \"postTypeId\", \"acceptedAnswerId\").show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|                body|\n",
      "+----+--------------------+\n",
      "|2001|&lt;p&gt;Sardinia...|\n",
      "|2002|&lt;p&gt;I am fro...|\n",
      "|2003|&lt;p&gt;La rispo...|\n",
      "|2004|&lt;p&gt;In itali...|\n",
      "|2005|&lt;p&gt;Mi Ã¨ st...|\n",
      "|2006|&lt;p&gt;âCa.â...|\n",
      "|2007|&lt;p&gt;Un chimi...|\n",
      "|2008|&lt;p&gt;&quot;sp...|\n",
      "|2009|&lt;p&gt;Ad occhi...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter/where\n",
    "itPostFiltered = itPostDF.where(\"id > 2000 and id < 2010\")\n",
    "itPostFiltered.select(\"id\", \"body\").show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[commentCount: int, lastActivityDate: timestamp, ownerUserId: bigint, body: string, score: int, creationDate: timestamp, viewCount: int, title: string, tags: string, answerCount: int, acceptedAnswerId: bigint, postTypeId: bigint, selected_id: bigint]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# withColumnRenamed\n",
    "itPostFiltered.withColumnRenamed(\"id\", \"selected_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#withColumn - new column\n",
    "itPostColAdd = itPostFiltered.withColumn(\n",
    "               \"score_div_answer\", \n",
    "                itPostFiltered['score']/itPostFiltered['answerCount']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------------+\n",
      "|score|answerCount|score_div_answer|\n",
      "+-----+-----------+----------------+\n",
      "|    2|          1|             2.0|\n",
      "|    2|          1|             2.0|\n",
      "+-----+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostColAdd.filter(itPostColAdd['score_div_answer'].isNotNull())\\\n",
    "            .select(\"score\", \"answerCount\", \"score_div_answer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|                body|\n",
      "+----+--------------------+\n",
      "|2009|&lt;p&gt;Ad occhi...|\n",
      "|2008|&lt;p&gt;&quot;sp...|\n",
      "|2007|&lt;p&gt;Un chimi...|\n",
      "|2006|&lt;p&gt;âCa.â...|\n",
      "|2005|&lt;p&gt;Mi Ã¨ st...|\n",
      "|2004|&lt;p&gt;In itali...|\n",
      "|2003|&lt;p&gt;La rispo...|\n",
      "|2002|&lt;p&gt;I am fro...|\n",
      "|2001|&lt;p&gt;Sardinia...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort\n",
    "itPostFiltered.sort(\"id\", ascending=False).select(\"id\", \"body\").show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  id|                body|\n",
      "+----+--------------------+\n",
      "|2009|&lt;p&gt;Ad occhi...|\n",
      "|2008|&lt;p&gt;&quot;sp...|\n",
      "|2007|&lt;p&gt;Un chimi...|\n",
      "|2006|&lt;p&gt;âCa.â...|\n",
      "|2005|&lt;p&gt;Mi Ã¨ st...|\n",
      "|2004|&lt;p&gt;In itali...|\n",
      "|2003|&lt;p&gt;La rispo...|\n",
      "|2002|&lt;p&gt;I am fro...|\n",
      "|2001|&lt;p&gt;Sardinia...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orderBy\n",
    "itPostFiltered.orderBy(\"id\", ascending=False).select(\"id\", \"body\").show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5\n",
    "\n",
    "Create a dataframe with\tfirst 3\tcolumns\tin the file\tto have: \n",
    "+ Id: long, not nullable\n",
    "+ commentDate: timestamp, nullable\n",
    "+ comment: string, nullable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile('./data/Italian_Stack_Exchange/italianComments.csv')\n",
    "data = data.map(lambda x: x.encode('utf8').split(\"~\"))\n",
    "data_selected = data.map(lambda x: [x[0], x[1], x[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['18',\n",
       "  '2013-11-05 20:39:30.453',\n",
       "  \"It's going to be really hard to answer in English...\",\n",
       "  '1',\n",
       "  '4',\n",
       "  '1']]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['18',\n",
       "  '2013-11-05 20:39:30.453',\n",
       "  \"It's going to be really hard to answer in English...\"]]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_selected.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stringToPost(r):\n",
    "  # r = row.encode('utf8').split(\"~\")\n",
    "  return Row(\n",
    "    long(r[0]), \n",
    "    toTimeSafe(r[1]),\n",
    "    r[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Schema\n",
    "theSchema = StructType([\n",
    "    StructField(\"Id\", LongType(), False)\n",
    "    , StructField(\"commentDate\", TimestampType(), True)\n",
    "    , StructField('comment', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert input data\n",
    "sqlContext = SQLContext(sc)\n",
    "rowRDD = data_selected.map(lambda x: stringToPost(x))\n",
    "df = sqlContext.createDataFrame(rowRDD, theSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| Id|         commentDate|             comment|\n",
      "+---+--------------------+--------------------+\n",
      "| 18|2013-11-05 20:39:...|It's going to be ...|\n",
      "|  6|2013-11-05 20:41:...|Why not &quot;IL ...|\n",
      "| 18|2013-11-05 20:43:...|    Yep, added that.|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = false)\n",
      " |-- commentDate: timestamp (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df_filtered = df.where(df['comment'].contains(\"@Daniele\"))\\\n",
    "                .where(df_filtered['commentDate'].contains(\"2013-11-07\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| Id|         commentDate|             comment|\n",
      "+---+--------------------+--------------------+\n",
      "| 37|2013-11-07 15:30:...|@Daniele B: I kno...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6\n",
    "- Users\tcan\tupvote and downvote for\tmarking\tcertain\tquestions useful or\tnot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile('./data/Italian_Stack_Exchange/italianVotes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'2657~135~2~2013-11-22 00:00:00.0']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stringToPost(row):\n",
    "  r = row.encode('utf8').split(\"~\")\n",
    "  return Row(\n",
    "    long(r[1]),\n",
    "    long(r[0]),\n",
    "    toIntSafe(r[2]),\n",
    "    toTimeSafe(r[3])\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theSchema = StructType([\n",
    "    StructField(\"id\", LongType(), False)\n",
    "    , StructField(\"postId\", LongType(), False)\n",
    "    , StructField(\"voteType\", IntegerType(), False)\n",
    "    , StructField(\"creationTime\", TimestampType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowRDD = data.map(lambda x: stringToPost(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "itVoteDF = sqlContext.createDataFrame(rowRDD, theSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-------------------+\n",
      "| id|postId|voteType|       creationTime|\n",
      "+---+------+--------+-------------------+\n",
      "|135|  2657|       2|2013-11-22 00:00:00|\n",
      "|142|  2658|       2|2013-11-22 00:00:00|\n",
      "|142|  2659|       1|2013-11-22 00:00:00|\n",
      "+---+------+--------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itVoteDF.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- postId: long (nullable = false)\n",
      " |-- voteType: integer (nullable = false)\n",
      " |-- creationTime: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itVoteDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-------------------+\n",
      "| id|postId|voteType|       creationTime|\n",
      "+---+------+--------+-------------------+\n",
      "|  2|    46|       1|2013-11-05 00:00:00|\n",
      "| 12|    53|       1|2013-11-05 00:00:00|\n",
      "| 36|   195|       1|2013-11-06 00:00:00|\n",
      "| 30|   220|       1|2013-11-06 00:00:00|\n",
      "| 55|   232|       1|2013-11-06 00:00:00|\n",
      "| 45|   181|       1|2013-11-06 00:00:00|\n",
      "| 52|   216|       1|2013-11-06 00:00:00|\n",
      "| 77|   364|       1|2013-11-07 00:00:00|\n",
      "| 83|   375|       1|2013-11-07 00:00:00|\n",
      "| 84|   380|       1|2013-11-07 00:00:00|\n",
      "+---+------+--------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itVoteDF.where(itVoteDF[\"voteType\"]== 1).sort(\"creationTime\").show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Functions with DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark SQL supports most of SQL functions\n",
    "\n",
    "#### **Scalar Functions**: \n",
    "- Return a single value for each row based on calculations on one of the columns\n",
    "- Math: abs, log, etc. \n",
    "- String: length, concat, trim, \n",
    "- Time: year, date_add, datediff, next_day\n",
    "\n",
    "#### **Aggregate Functions**: \n",
    "- Return a single value for a group of rows\n",
    "- Can be used in combination with **`groupBy()`**\n",
    "- E.g. \n",
    "```python\n",
    "df.groupBy().avg(<column_name>)\n",
    "df.select(avg(df.column_name))\n",
    "```\n",
    "\n",
    "#### **Window Functions**: \n",
    "- Return several values for a group of rows\n",
    "- Unlike aggregate\tfunctions,\tthey don’t\tgroup rows into a single\t output\tper\tgroup\n",
    "\n",
    "![](window_func.png)\n",
    "\n",
    "#### **User-defined Functions (UDF)**: \n",
    "- Generate custom scalar or aggregate functions\n",
    "- udf_name = udf(lambda function definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7 - Aggregate Functions\n",
    "\n",
    "- Add a\tcolumn\tcalled\t“duration”,\twhich is difference\tbetween\t ‘lastActivityDate’\tand\t‘creationDate’ and\tsort the DataFrame by ‘duration’ in\t descending\torder.\n",
    "- Calculate\tan\taverage\tscore per owner\tand\tsort data by average scores in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|duration|    lastActivityDate|        creationDate|\n",
      "+--------+--------------------+--------------------+\n",
      "|     303|2014-09-11 14:37:...|2013-11-12 13:34:...|\n",
      "|     301|2014-09-09 08:54:...|2013-11-12 11:03:...|\n",
      "|     296|2014-09-12 10:55:...|2013-11-20 16:42:...|\n",
      "|     292|2014-08-31 20:19:...|2013-11-12 12:04:...|\n",
      "|     291|2014-08-24 16:01:...|2013-11-06 22:12:...|\n",
      "|     286|2014-09-05 21:35:...|2013-11-23 14:54:...|\n",
      "|     281|2014-08-19 15:39:...|2013-11-11 18:52:...|\n",
      "|     278|2014-08-12 12:47:...|2013-11-07 17:43:...|\n",
      "|     278|2014-08-18 18:06:...|2013-11-13 19:04:...|\n",
      "|     277|2014-08-31 20:18:...|2013-11-27 10:50:...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new column duration\n",
    "itPostDF.withColumn('duration',\n",
    "                   datediff('lastActivityDate','creationDate'))\\\n",
    "        .sort('duration', ascending=False)\\\n",
    "        .select('duration', 'lastActivityDate', 'creationDate')\\\n",
    "        .show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|ownerUserId|avg(score)|\n",
      "+-----------+----------+\n",
      "|        570|      15.0|\n",
      "|          6|      15.0|\n",
      "|        730|      12.0|\n",
      "|        729|      11.0|\n",
      "|        154|      11.0|\n",
      "|        220|      10.0|\n",
      "|        217|      10.0|\n",
      "|        445|       9.0|\n",
      "|        116|       9.0|\n",
      "|        656|       9.0|\n",
      "+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDF.groupBy('ownerUserId').avg('score')\\\n",
    "        .sort('avg(score)', ascending=False)\\\n",
    "        .show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8 - Window Functions\n",
    "\n",
    "- For each\tquestion, find the id of its owner’s previous question by\tcreation date.\n",
    "- For each question, find the id of its ownser's previous and next question by creation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+--------------------+----+\n",
      "|ownerUserId|  id|        creationDate|prev|\n",
      "+-----------+----+--------------------+----+\n",
      "|          4|1637|2014-01-24 06:51:...|null|\n",
      "|          8|   1|2013-11-05 20:22:...|null|\n",
      "|          8| 112|2013-11-08 13:14:...|   1|\n",
      "|          8|1192|2013-11-11 21:01:...| 112|\n",
      "|          8|1276|2013-11-15 16:09:...|1192|\n",
      "|          8|1321|2013-11-20 16:42:...|1276|\n",
      "|          8|1365|2013-11-23 09:09:...|1321|\n",
      "|         12|  11|2013-11-05 21:30:...|null|\n",
      "|         12|  17|2013-11-05 22:17:...|  11|\n",
      "|         12|  18|2013-11-05 22:34:...|  17|\n",
      "+-----------+----+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDF.filter(itPostDF.postTypeId == 1)\\\n",
    "        .select('ownerUserId', 'id', 'creationDate',\n",
    "                lag(itPostDF.id, 1)\\\n",
    "                .over(Window.partitionBy(itPostDF.ownerUserId)\\\n",
    "                            .orderBy('ownerUserId', 'creationDate'))\\\n",
    "                .alias('prev'))\\\n",
    "        .orderBy('ownerUserId', 'creationDate')\\\n",
    "        .show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "winDf = itPostDf.filter(postsDf.postTypeId == 1)\\\n",
    "                .select(postsDf.ownerUserId, postsDf.id, postsDf.creationDate,\n",
    "                        lag(postsDf.id, 1)\\\n",
    "                        .over(Window.partitionBy(postsDf.ownerUserId)\\\n",
    "                            .orderBy(postsDf.creationDate))\\\n",
    "                            .alias(\"prev\"), lead(postsDf.id, 1)\\\n",
    "                        .over(Window.partitionBy(postsDf.ownerUserId)\\\n",
    "                            .orderBy(postsDf.creationDate)).alias(\"next\"))\\\n",
    "                .orderBy(postsDf.ownerUserId, postsDf.id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9 - User-defined Functions\n",
    "\n",
    "Create a UDF called `countTags` which count the number of `&lt;` and count the number of them in the tags column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countTags = udf(lambda x: x.count(\"&lt;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|  id|<lambda>(tags)|\n",
      "+----+--------------+\n",
      "|1165|             0|\n",
      "|1166|             1|\n",
      "|1167|             0|\n",
      "|1168|             3|\n",
      "|1169|             0|\n",
      "|1170|             0|\n",
      "|1171|             0|\n",
      "|1172|             0|\n",
      "|1173|             2|\n",
      "|1174|             0|\n",
      "+----+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDF.select('id', countTags('tags')).show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 10 - User-defined Functions\n",
    "\n",
    "Write a UDF called scoreString which discretize scores: \n",
    "- Lower than 10: Low\n",
    "- Between 10 and 20: Med\n",
    "- Higher than 20: High\n",
    "\n",
    "Show the score Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_discrete(x):\n",
    "    if x < 10: return 'low'\n",
    "    elif (x >=10 and x <= 20): return 'med'\n",
    "    elif x > 20: return 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreString = udf(lambda x: score_discrete(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------------+\n",
      "|  id|score|<lambda>(score)|\n",
      "+----+-----+---------------+\n",
      "|1165|   23|           high|\n",
      "|1166|    1|            low|\n",
      "|1167|    5|            low|\n",
      "|1168|   11|            med|\n",
      "|1169|    3|            low|\n",
      "|1170|    8|            low|\n",
      "|1171|    3|            low|\n",
      "|1172|    1|            low|\n",
      "|1173|    5|            low|\n",
      "|1174|    5|            low|\n",
      "+----+-----+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDF.select('id', 'score', scoreString('score')).show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Data\n",
    "\n",
    "- Return `GroupedData` object\n",
    "- Can use an aggregate function or `agg(list_of_agg_func)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+------------------+\n",
      "|ownerUserId|max(score)|min(score)|        avg(score)|\n",
      "+-----------+----------+----------+------------------+\n",
      "|        270|         1|         1|               1.0|\n",
      "|        730|        12|        12|              12.0|\n",
      "|        720|         1|         1|               1.0|\n",
      "|         19|        10|        -1| 3.076923076923077|\n",
      "|        348|         5|         5|               5.0|\n",
      "|        415|         5|         1|2.1666666666666665|\n",
      "|        656|         9|         9|               9.0|\n",
      "|        736|         1|         1|               1.0|\n",
      "|         22|        19|         0| 4.886363636363637|\n",
      "|        198|         5|         5|               5.0|\n",
      "+-----------+----------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDF.groupBy('ownerUserId')\\\n",
    "        .agg(max('score'), min('score'), avg('score'))\\\n",
    "        .show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Data\n",
    "\n",
    "- Form: `join(df, condition, join_type)`\n",
    "- Join type: `inner` (default), `outer`, `left_outer`, `right_outer`, `leftsemi`\n",
    "- Example: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 13 - Joining\n",
    "\n",
    "Join Posts DataFrame with Vote DataFrame with\titPostsDFStruct.id == itVotesDFStruct.postId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+-----+--------------------+-----------+----------------+----------+----+----+------+--------+-------------------+\n",
      "|commentCount|    lastActivityDate|ownerUserId|                body|score|        creationDate|viewCount|title|                tags|answerCount|acceptedAnswerId|postTypeId|  id|  id|postId|voteType|       creationTime|\n",
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+-----+--------------------+-----------+----------------+----------+----+----+------+--------+-------------------+\n",
      "|           1|2013-11-06 00:22:...|         18|&lt;p&gt;&quot;De...|    3|2013-11-06 00:05:...|     null| null|                    |       null|            null|         2|  26|   2|    26|       2|2013-11-05 00:00:00|\n",
      "|           0|2013-11-07 23:59:...|         56|&lt;p&gt;I know t...|   10|2013-11-06 01:16:...|      143| null|    &lt;pronouns&gt;|          2|              30|         1|  29|   8|    29|       2|2013-11-05 00:00:00|\n",
      "|           1|2014-03-05 16:35:...|         12|&lt;blockquote&gt...|    1|2014-02-15 15:13:...|      125| null|&lt;word-choice&g...|          1|            null|         1|1697|1158|  1697|       2|2013-11-10 00:00:00|\n",
      "|           0|2014-03-12 14:02:...|        490|&lt;p&gt;The seco...|    0|2014-03-12 14:02:...|     null| null|                    |       null|            null|         2|1806|1181|  1806|       2|2013-11-11 00:00:00|\n",
      "|          11|2014-05-07 16:57:...|         37|&lt;p&gt;No poeti...|   11|2014-05-07 16:24:...|     null| null|                    |       null|            null|         2|1950|1191|  1950|       2|2013-11-12 00:00:00|\n",
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+-----+--------------------+-----------+----------------+----------+----+----+------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itPostDF.join(itVoteDF, itPostDF.id == itVoteDF.postId, 'inner')\\\n",
    "        .show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register DataFrame in the Table Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can reference a DataFrame by its name by registering the DataFrame as a table\n",
    "- Spark stores the table definition in the table catalog\n",
    "- Save as table (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example:\n",
    "\n",
    "```\n",
    ".registerTempTable(<table_name>) # disappear with the Spark session\n",
    "\n",
    ".write.saveAsTable(<table_name>) # register table permanently\n",
    "```\n",
    "- By writing, the table definitions will survive as the application restarts and are persistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itPostDF.write.saveAsTable(\"Post\")\n",
    "itVoteDF.write.saveAsTable(\"Votes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once the DataFrame is registered as a table, you can query its data using SQL expressions\n",
    "- Use `sqlContext.sql(query)`\n",
    "- Can also use spark-sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 15\n",
    "Run\t“select\t* from posts” query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+-----+--------------------+-----------+----------------+----------+----+\n",
      "|commentCount|    lastActivityDate|ownerUserId|                body|score|        creationDate|viewCount|title|                tags|answerCount|acceptedAnswerId|postTypeId|  id|\n",
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+-----+--------------------+-----------+----------------+----------+----+\n",
      "|           4|2013-11-11 18:21:...|         17|&lt;p&gt;The infi...|   23|2013-11-10 19:37:...|     null| null|                    |       null|            null|         2|1165|\n",
      "|           5|2013-11-10 20:31:...|         12|&lt;p&gt;Come cre...|    1|2013-11-10 19:44:...|       61| null| &lt;word-choice&gt;|          1|            null|         1|1166|\n",
      "|           2|2013-11-10 20:31:...|         17|&lt;p&gt;Il verbo...|    5|2013-11-10 19:58:...|     null| null|                    |       null|            null|         2|1167|\n",
      "|           1|2014-07-25 13:15:...|        154|&lt;p&gt;As part ...|   11|2013-11-10 22:03:...|      187| null|&lt;english-compa...|          4|            1170|         1|1168|\n",
      "|           0|2013-11-10 22:15:...|         70|&lt;p&gt;&lt;em&g...|    3|2013-11-10 22:15:...|     null| null|                    |       null|            null|         2|1169|\n",
      "+------------+--------------------+-----------+--------------------+-----+--------------------+---------+-----+--------------------+-----------+----------------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from post\").show(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Saving Data using SparkSQL\n",
    "\n",
    "#### Datatypes\n",
    "\n",
    "- **JSON**: \n",
    "    + Spark can automatically infer a JSON schema\n",
    "    + `spark.read.json(<file_name>)`\n",
    "    \n",
    "    \n",
    "- **ORC**: Optimixed Row Columnar\n",
    "    + Columnar format\n",
    "    + `sqlContext.read.format('orc').load(<file_name>)`\n",
    "    \n",
    "    \n",
    "- **Parquet**\n",
    "    + Columnar file-based storage format optimized for relational access\n",
    "    + Designed to be independent of any specific framework and free of unnecessary dependencies\n",
    "    + `spark.read.parquet(<file_name>)`\n",
    "    \n",
    "    \n",
    "- **Relational DB and other DB with JDBC**\n",
    "    + Use JDBC drivers (Postgres and others (MySQL))\n",
    "    + Spark can access several popular databases using either Hadoop connectors or custom\n",
    "Spark connectors.\n",
    "    + Spark can load data from any relational databases that supports Java Database\n",
    "Connectivity (JDBC) including MySQL, Postgres, and other systems.\n",
    "    + Download a Postgres JDBC jar from https://jdbc.postgresql.org/download.html\n",
    "    + `pyspark --driver-class-path Postgres_JDBC_location`\n",
    "    + Format: \n",
    "    ```python\n",
    "    # where <propery_list> is a dict includes user, pass and driver\n",
    "    from pyspark.sql import DataFrameReader\n",
    "    df = DataFrameRead(sqlContext).jdbc(\n",
    "                                    url = 'jdbc:<URL>',\n",
    "                                    table = '<table_name>',\n",
    "                                    properties = <property_list>\n",
    "                                    ) \n",
    "    ```\n",
    "    \n",
    "#### Saving \n",
    "- `.write.format(<format>).saveAsTable(<name>)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 16 \n",
    "\n",
    "Read word_bank_project.json as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# world_bank_prj = spark.read.json(\"s3n://usfca-msan694/world_bank_project.json\")\n",
    "world_bank_prj = spark.read.json(\"./data/world_bank_project.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+----------------+--------------------+------------------------+--------+-----------+-------+----------+--------------------+--------------------+----------------+---------------+--------------------+--------------------+--------------------+--------------------+-----------+--------+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+------+------+--------------------+--------------------+--------------------+-----------+---------+------------+--------------------+\n",
      "|                 _id|approvalfy|board_approval_month|   boardapprovaldate|            borrower|         closingdate|    country_namecode|countrycode|         countryname|countryshortname|               docty|envassesmentcategorycode|grantamt|ibrdcommamt|     id|idacommamt|           impagency|        lendinginstr|lendinginstrtype|lendprojectcost| majorsector_percent|   mjsector_namecode|             mjtheme|    mjtheme_namecode|mjthemecode|prodline|        prodlinetext|productlinetype|    project_abstract|        project_name|         projectdocs|projectfinancialtype|projectstatusdisplay|          regionname|              sector|             sector1|             sector2|             sector3|             sector4|     sector_namecode| sectorcode|source|status|supplementprojectflg|              theme1|      theme_namecode|  themecode| totalamt|totalcommamt|                 url|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+----------------+--------------------+------------------------+--------+-----------+-------+----------+--------------------+--------------------+----------------+---------------+--------------------+--------------------+--------------------+--------------------+-----------+--------+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+------+------+--------------------+--------------------+--------------------+-----------+---------+------------+--------------------+\n",
      "|[52b213b38594d8a2...|      1999|            November|2013-11-12T00:00:00Z|FEDERAL DEMOCRATI...|2018-07-07T00:00:00Z|Federal Democrati...|         ET|Federal Democrati...|        Ethiopia|Project Informati...|                       C|       0|          0|P129828| 130000000|MINISTRY OF EDUCA...|Investment Projec...|              IN|      550000000|[[Education,46], ...|[[EX,Education], ...| [Human development]|[[8,Human develop...|       8,11|      PE|            IBRD/IDA|              L|[The development ...|Ethiopia General ...|[[28-AUG-2013,PID...|                 IDA|              Active|              Africa|[[Primary educati...|[Primary educatio...|[Secondary educat...|[Public administr...|[Tertiary educati...|[[EP,Primary educ...|ET,BS,ES,EP|  IBRD|Active|                   N|[Education for al...|[[65,Education fo...|         65|130000000|   130000000|http://www.worldb...|\n",
      "|[52b213b38594d8a2...|      2015|            November|2013-11-04T00:00:00Z|GOVERNMENT OF TUN...|                null|Republic of Tunis...|         TN| Republic of Tunisia|         Tunisia|Project Informati...|                       C| 4700000|          0|P144674|         0| MINISTRY OF FINANCE|Specific Investme...|              IN|        5700000|[[Public Administ...|[[BX,Public Admin...|[Economic managem...|[[1,Economic mana...|        1,6|      RE|Recipient Execute...|              L|                null|TN: DTF Social Pr...|[[29-MAR-2013,PID...|               OTHER|              Active|Middle East and N...|[[Public administ...|[Public administr...|[General public a...|                null|                null|[[BS,Public admin...|      BZ,BS|  IBRD|Active|                   N|[Other economic m...|[[24,Other econom...|      54,24|        0|     4700000|http://www.worldb...|\n",
      "|[52b213b38594d8a2...|      2014|            November|2013-11-01T00:00:00Z|MINISTRY OF FINAN...|                null|         Tuvalu!$!TV|         TV|              Tuvalu|          Tuvalu|Resettlement Plan...|                       B|       0|          0|P145310|   6060000|MINISTRY OF TRANS...|Investment Projec...|              IN|        6060000|[[Transportation,...|[[TX,Transportati...|[Trade and integr...|[[5,Trade and int...|   5,2,11,6|      PE|            IBRD/IDA|              L|                null|Tuvalu Aviation I...|[[21-OCT-2013,RPL...|                 IDA|              Active|East Asia and Pac...|[[Rural and Inter...|[Rural and Inter-...|                null|                null|                null|[[TI,Rural and In...|         TI|  IBRD|Active|                   Y|[Regional integra...|[[47,Regional int...|52,81,25,47|  6060000|     6060000|http://www.worldb...|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+----------------+--------------------+------------------------+--------+-----------+-------+----------+--------------------+--------------------+----------------+---------------+--------------------+--------------------+--------------------+--------------------+-----------+--------+--------------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+------+------+--------------------+--------------------+--------------------+-----------+---------+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_bank_prj.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postDF = sqlContext.sql(\"select * from post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postDF.write.format('json').saveAsTable('post_json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
