{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Review "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.textFile('./data/USF_Mission.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.map(lambda x: x.split()).flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = data.map(lambda x: (x,1))\n",
    "word_count = word_count.reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top5_word = word_count.sortBy(lambda x: x[1], ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and : 25\n",
      "the : 22\n",
      "of : 19\n",
      "to : 18\n",
      "in : 13\n"
     ]
    }
   ],
   "source": [
    "for w in top5_word: \n",
    "    print \"%s : %s\" % (w[0], w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition: \n",
    "\n",
    "- For parallel collections, users can specify the number of partitions to cut the RDD into.\n",
    "\n",
    "- Form: \n",
    "```\n",
    "sc.parallelize(data).transformation(...,partitionSize)\n",
    "```\n",
    "\n",
    "- How to check number of partitions: \n",
    "    1. `getNumPartitions()`\n",
    "    2. `glom()`\n",
    "    \n",
    "    \n",
    "- Thumb Rule for # Partitions: Twice of the number of CPU cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "- Sending data back and forth between executors on parallelized distributed system causes network traffic.\n",
    "- Better to place data that can minimize shuffling and improve performance. \n",
    "- Workload might not be evenly distributed in some partitions.\n",
    "- May cause efficiency or memory issues.\n",
    "- Example: \n",
    "```python\n",
    "lines = sc.textFile(\"filtered_registered_business_sf.csv\")\n",
    "sf_business = lines.map(lambda x: (x.split(\",\")[0],x)).persist()\n",
    "```\n",
    "- Efficient if tuples with the same key are in the same partition\n",
    "    + e.g. reduceByKey: have to shuffle the data -> costly \n",
    "    + Solution: putting the data with the same key in the same partition can reduce shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define “data” which is numbers between 1 and 9 and load them into random number of partitions (between 1 and 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random as rand \n",
    "data = sc.parallelize(range(1,10), rand.randint(1,5))\n",
    "data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5], [6, 7], [8, 9]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "- Parallelize data into 3 partitioners.\n",
    "- Perform reduceByKey(lambda x,y : x+y) and check the number of partitioner.\n",
    "- Perform reduceByKey(lambda x,y : x+y, NUM) and check the number of partitioner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', 3)], [('b', 4)], [('a', 5)]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('a',3),('b',4),('a',5)]\n",
    "data = sc.parallelize(data, 3)\n",
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[], [('b', 4)], [('a', 8)]]\n"
     ]
    }
   ],
   "source": [
    "data1 = data.reduceByKey(lambda x, y: x+y)\n",
    "print(data1.getNumPartitions())\n",
    "print(data1.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[[('a', 8)], [], [], [('b', 4)], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "data2 = data.reduceByKey(lambda x, y: x+y, 8)\n",
    "print(data2.getNumPartitions())\n",
    "print(data2.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: **Partitioner**\n",
    "- Pair RDDs. : partitionBy()\n",
    "- Defines how the elements in a key-value pair RDD are **partitioned by key**.\n",
    "- Manage data commonly accessible together on the **same node**, i.e. same partition\n",
    "- Organize data for minimizing network traffic/communication to improve\n",
    " performance.\n",
    "- Maps each key to a partition ID, from 0 to numPartitions - 1. \n",
    "\n",
    "\n",
    "- What are the rules that detemrine where the data should go? \n",
    "- Types : HashPartitioner, RangePartitioner, Custom Partitioner. \n",
    "    1. **HashPartitioner**: partitioner using a hash value of a key.\n",
    "          `.partitionBy(N)` uses HashPartitioner by default.\n",
    "    2. **RangePartitioner**: partitioner partitioning sorted RDDs into roughly equal ranges. \n",
    "    3. **CustomPartitioner**: User-defined paritioner.\n",
    "\n",
    "\n",
    "- Example\n",
    "```python\n",
    "def custom_partitioner(key): return hash(key + 10)\n",
    "pair_rdd.partitonBy(N, custom_partitioner)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- partitionBy() : Spark know that it is hash-partitioned.\n",
    "- **persist() : Without persisting an RDD after partitioning, it will cause subsequent uses of the RDD to repeat the partitioning of the data.**\n",
    "- Much Faster : Only shuffle supervisor data, sending supervisor data with each particular key to the machine that has the corresponding key of business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 \n",
    "Try different types of operations to change the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([('a',3),('b',4),('a',5), ('c',3), ('b',5)], 8)\n",
    "data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', 3), ('a', 5)], [], [('c', 3)], [('b', 4), ('b', 5)], [], [], [], []]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HashPartitioner - using hash value of a key\n",
    "data1 = data.partitionBy(8) # partitionBy uses HashPartitioner by default.\n",
    "data1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('a', 3), ('a', 5)], [], [], [('b', 4), ('b', 5)], [], [], [('c', 3)], []]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RangePartitioner - sorting key\n",
    "data2 = data.sortByKey()\n",
    "data2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CustomPartitioner\n",
    "def custom_partitioner(key): return hash(key + 10)\n",
    "\n",
    "data3 = data.partitionBy(8, custom_partitioner) #partitionBy uses HashPartitioner by default.\n",
    "# data3.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4\n",
    "Join “filtered_registered_business_sf.csv” and “supervisor_sf.csv” efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biz_raw = sc.textFile('./data/filtered_registered_business_sf.csv')\n",
    "supervisor_raw = sc.textFile('./data/supervisor_sf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# biz = biz_raw.flatMap(lambda l: l.split(\"\\n\")).map(lambda x: x.split(\",\"))\n",
    "biz = biz_raw.map(lambda x: x.split(\",\"))\n",
    "supervisor = supervisor_raw.flatMap(lambda l: l.split(\"\\n\")).map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198566, 75)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz.count(), supervisor.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'94123',\n",
       "  u'Tournahu George L',\n",
       "  u'3301 Broderick St',\n",
       "  u'San Francisco',\n",
       "  u'CA']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'94102', u'8'], [u'94102', u'6'], [u'94102', u'3']]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervisor.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 ms, sys: 2.77 ms, total: 15.1 ms\n",
      "Wall time: 25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined = biz.join(supervisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'94123', (u'Tournahu George L', u'2'))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with partitionBy\n",
    "biz2 = biz_raw.map(lambda x: (x.split(\",\")[0],x)).partitionBy(5).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'94109',\n",
       " u'94109,Stephens Institute Inc,1835-49 Van Ness Ave,San Francisco,CA')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 ms, sys: 5.52 ms, total: 19.6 ms\n",
      "Wall time: 29.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined = biz2.join(supervisor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [(1, 2), (1, 3)], [(2, 3)], [], []]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(1,2), (1,3), (2,3)]).partitionBy(5).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: too many values to unpack\n"
     ]
    }
   ],
   "source": [
    "# partitionBy does not work with multiple-value tuples, only tuple with 2 values (a,b)\n",
    "try: \n",
    "    sc.parallelize([(1,2,3), (1,3,4), (2,3,5)]).partitionBy(5).collect()\n",
    "except: \n",
    "    print(\"ValueError: too many values to unpack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5\n",
    "Create a custom practitioner using\n",
    "1. hash value of the key.\n",
    "2. hash value of the key + 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 'a')], [(2, 'b'), (3, 'c')], [(1, 'd')], [(1, 'e'), (2, 'f')]]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([(1, 'a'), (2, 'b'), (3, 'c'), (1, 'd'), (1, 'e'), (2, 'f')])\n",
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CustomPartitioner\n",
    "def custom_partitioner1(key): return hash(key)\n",
    "def custom_partitioner2(key): return hash(key + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [(1, 'a'), (1, 'd'), (1, 'e')], [(2, 'b'), (2, 'f')], [(3, 'c')]]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data.partitionBy(4, custom_partitioner1) #partitionBy uses HashPartitioner by default.\n",
    "data1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(2, 'b'), (2, 'f')], [(3, 'c')], [], [(1, 'a'), (1, 'd'), (1, 'e')]]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data.partitionBy(4, custom_partitioner2) #partitionBy uses HashPartitioner by default.\n",
    "data2.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations and Partitioner\n",
    "*Operations benefiting from partitioning* \n",
    "\n",
    "- Operations involving **shuffling data by key** across the network\n",
    "    + join(), leftOuterJoin(), rightOuterJoin()\n",
    "    + groupByKey(), reduceByKey(), combineByKey()\n",
    "    + lookUp(), etc.\n",
    "\n",
    "\n",
    "- Operations returns RDDs with **known partitioning information**. \n",
    "    + sortByKey(): range-partition\n",
    "    + groupByKey(): hash-partition\n",
    "    + most of them use hash-partitioning\n",
    "\n",
    "\n",
    "- Operations forget the parent’s partitioning information.\n",
    "    + map(): because it can theoretically modify the key of each record.\n",
    "    + map() does not really benefit from reducing shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning RDDs\n",
    "Change the partitioning to distribute the workload more efficiently or avoid memory problems.\n",
    "\n",
    "- **`repartition(numPartitions: Int)`**\n",
    "    - basically move data from one partition to the other\n",
    "    - does not guarantee minimize number of shuffling\n",
    "    - shuffle data across the network to create a new set of partitions.\n",
    "\n",
    "\n",
    "- **`coalesce(numPartitions: Int, shuffle = false)`**\n",
    "    - guaranteed minimize number of shufflling\n",
    "    - **optimized** version of repartition() \n",
    "    – avoid data movement and reduce the number of RDD partitions.\n",
    "    - match the locality as much as possible, but try to balance partitions across the machines.\n",
    "    - as number of partition increases, the performance of `coalesce()` is not much different from `repartition()`\n",
    "    - by default, **`shuffle=False`**, the number of partitions would be the same as the original (i.e. no repartitioning)\n",
    "    - with **`shuffle=True`**, the output is evenly distributed amongst the partitions (and your also able to increase the # of partitions if you wanted)\n",
    "\n",
    "#### repartition() vs. partitionBy()\n",
    "`repartition` already exists in RDDs, and does **not handle partitioning by key** (or by any other criterion except Ordering). Now PairRDDs add the notion of keys and subsequently add another method that allows to partition by that key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6 \n",
    "\n",
    "Compare .coalesce() and .repartition().\n",
    "- Which one shuffles data less?\n",
    "- Can the number of partitions smaller than its parent’s number of partitions? ==> Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 'a')], [(2, 'b'), (3, 'c')], [(1, 'd')], [(1, 'e'), (2, 'f')]]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([(1, 'a'), (2, 'b'), (3, 'c'), (1, 'd'), (1, 'e'), (2, 'f')])\n",
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.33 ms, sys: 2.83 ms, total: 10.2 ms\n",
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data1 = data.repartition(5)\n",
    "data1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 ms, sys: 3.66 ms, total: 14 ms\n",
      "Wall time: 120 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data2 = data.coalesce(5, shuffle=True)\n",
    "data2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 'a'), (2, 'b'), (3, 'c'), (1, 'd'), (1, 'e'), (2, 'f')], []]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = data.repartition(2)\n",
    "data3.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7 \n",
    "- Determine a measure of importance (a “rank”) to each document based on how many documents have links to it.\n",
    "- Applications : Rank web pages, influential users in a social network, etc.\n",
    "- (Also could be implemented using GraphX.)\n",
    "\n",
    "Write a page rank algorithms where data = [(1,[2,3,4]), (2,[1,3]), (3,[4])] where the format is (URL, [LIST OF URLS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition-specific Methods \n",
    "Operations specifically designed to interact with partitions as atomic units.\n",
    "- `foreachPartition()` : Apply a function to each partition of an RDD.\n",
    "- `glom()` : Return all elements within each partition.\n",
    "- `lookup(key)`: Return values for the key using the partitioner to narrow its search to only the partitions where the key would present.\n",
    "- `mapPartitions()`: Return a new RDD by applying a function to each partition of the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Dependencies / Linage\n",
    "\n",
    "A dependency between an old and a new RDD is created, every time a transformation is performed on an RDD.\n",
    "- Spark’s execution model is based on directed acyclic graphs (DAGs), where nodes are RDDs and edges are dependencies.\n",
    "- The new RDD depend on the old RDD.\n",
    "- **RDD Resilience** - As Spark records the linage of each RDD, any RDDs can be reconstructed to the state it was at the time of the failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Dependency Types\n",
    "- **Narrow** – When no data shuffle between partitions is required.\n",
    "- **Wide** - When it requires shuffle when joining RDDs (more expensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list = [rand.randrange(10) for x in range(500)]\n",
    "listrdd = sc.parallelize(list, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = listrdd.map(lambda x: (x, x*x)) # narrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced = pairs.reduceByKey(lambda v1, v2: v1+v2) # wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalrdd = reduced.mapPartitions(lambda itr: \n",
    "                                [\"K=\"+str(k) + \", V=\"+str(v) for (k,v) in itr]) # narrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K=0, V=0',\n",
       " 'K=5, V=1375',\n",
       " 'K=1, V=50',\n",
       " 'K=6, V=1512',\n",
       " 'K=2, V=244',\n",
       " 'K=7, V=1666',\n",
       " 'K=8, V=3136',\n",
       " 'K=3, V=468',\n",
       " 'K=9, V=4455',\n",
       " 'K=4, V=896']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalrdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark stages and tasks.\n",
    "- Every job is divided into **stages** based on the points **where shuffles occur**.\n",
    "- For each stage, tasks are created and sent to the executors.\n",
    "- After all tasks of a particular stage complete, the **driver creates tasks** for the next stage and sends them to the executors.\n",
    "- The results of each stage are saved on disk as **intermediate files** on executor machines\n",
    "- During the next stage, each partition receives data from these intermediate files belonging to it, and the execution is continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`toDebugString()`\n",
    "- Shows a textual representation of RDD dependencies.\n",
    "- The RDDs in the output appears in reverse order.\n",
    "- Useful in trying to minimize the number of shuffles.\n",
    "- The numbers in parentheses show the number of partitions of the corresponding RDD.\n",
    "- Every time you see a `ShuffleRDD` in the lineage chain, you can be sure that a shuffle will be performed at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5) PythonRDD[535] at collect at <ipython-input-183-036dee88a049>:1 []\n",
      " |  MapPartitionsRDD[533] at mapPartitions at PythonRDD.scala:427 []\n",
      " |  ShuffledRDD[532] at partitionBy at <unknown>:0 []\n",
      " +-(5) PairwiseRDD[531] at reduceByKey at <ipython-input-179-f90113ad700e>:1 []\n",
      "    |  PythonRDD[530] at reduceByKey at <ipython-input-179-f90113ad700e>:1 []\n",
      "    |  ParallelCollectionRDD[529] at parallelize at PythonRDD.scala:480 []\n"
     ]
    }
   ],
   "source": [
    "# indent \"+-\" means shuffling\n",
    "# (5) is the number of partitions\n",
    "# 2 stages for 1 shuffle, 3 stages for 2 shuffles\n",
    "print finalrdd.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5) PythonRDD[556] at RDD at PythonRDD.scala:48 []\n",
      " |  MapPartitionsRDD[555] at mapPartitions at PythonRDD.scala:427 []\n",
      " |  ShuffledRDD[554] at partitionBy at <unknown>:0 []\n",
      " +-(5) PairwiseRDD[553] at sortByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "    |  PythonRDD[552] at sortByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "    |  MapPartitionsRDD[549] at mapPartitions at PythonRDD.scala:427 []\n",
      "    |  ShuffledRDD[548] at partitionBy at <unknown>:0 []\n",
      "    +-(5) PairwiseRDD[547] at reduceByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "       |  PythonRDD[546] at reduceByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "       |  ParallelCollectionRDD[529] at parallelize at PythonRDD.scala:480 []\n"
     ]
    }
   ],
   "source": [
    "order_finalRdd = pairs.reduceByKey(lambda v1, v2: v1+v2).sortByKey()\n",
    "print order_finalRdd.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5) MapPartitionsRDD[566] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |  CoalescedRDD[565] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |  ShuffledRDD[564] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " +-(5) MapPartitionsRDD[563] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[562] at RDD at PythonRDD.scala:48 []\n",
      "    |  MapPartitionsRDD[555] at mapPartitions at PythonRDD.scala:427 []\n",
      "    |  ShuffledRDD[554] at partitionBy at <unknown>:0 []\n",
      "    +-(5) PairwiseRDD[553] at sortByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "       |  PythonRDD[552] at sortByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "       |  MapPartitionsRDD[549] at mapPartitions at PythonRDD.scala:427 []\n",
      "       |  ShuffledRDD[548] at partitionBy at <unknown>:0 []\n",
      "       +-(5) PairwiseRDD[547] at reduceByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "          |  PythonRDD[546] at reduceByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "          |  ParallelCollectionRDD[529] at parallelize at PythonRDD.scala:480 []\n"
     ]
    }
   ],
   "source": [
    "print order_finalRdd.repartition(5).toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint \n",
    "\n",
    "- When caching the RDDs, it will remember all the history in the Linage Graph / Dependencies. Keeping all will lead to overhead\n",
    "- Use **checkpoint** - saving the data itself on the disk\n",
    "- Persist RDDs to disk.\n",
    "- After checkpointing, the RDD ’s dependencies are erased, as well as the information about its parent(s), because they won’t be needed for its recomputation any more.\n",
    "- cf. persist(), cache() : Keeps RDD’s dependencies.\n",
    "\n",
    "\n",
    "- Example: \n",
    "\n",
    "```python\n",
    "sc.setCheckpointDir(\"dir\") # sets the directory where RDDs will be checkpointed.\n",
    "RDD.checkpoint() # will be triggered once an action is called.\n",
    "RDD.action() # After checkpointing, the RDD linage including its parents’ information will be removed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalRdd = order_finalRdd.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5) MapPartitionsRDD[578] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |  CoalescedRDD[577] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |  ShuffledRDD[576] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " +-(5) MapPartitionsRDD[575] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[574] at RDD at PythonRDD.scala:48 []\n",
      "    |  MapPartitionsRDD[555] at mapPartitions at PythonRDD.scala:427 []\n",
      "    |  ShuffledRDD[554] at partitionBy at <unknown>:0 []\n",
      "    +-(5) PairwiseRDD[553] at sortByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "       |  PythonRDD[552] at sortByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "       |  MapPartitionsRDD[549] at mapPartitions at PythonRDD.scala:427 []\n",
      "       |  ShuffledRDD[548] at partitionBy at <unknown>:0 []\n",
      "       +-(5) PairwiseRDD[547] at reduceByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "          |  PythonRDD[546] at reduceByKey at <ipython-input-188-d06ec506fdd0>:1 []\n",
      "          |  ParallelCollectionRDD[529] at parallelize at PythonRDD.scala:480 []\n"
     ]
    }
   ],
   "source": [
    "print finalRdd.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "sc.setCheckpointDir(\"checkpoint\")\n",
    "finalRdd.checkpoint()\n",
    "print finalRdd.isCheckpointed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "file:/Users/ThyKhueLy/msan697/inclass/checkpoint/51ba1be1-c290-4e25-9d19-5f34017eac01/rdd-578\n"
     ]
    }
   ],
   "source": [
    "finalRdd.count() # Have to call an action before calling isCheckPointed\n",
    "print finalRdd.isCheckpointed()\n",
    "print finalRdd.getCheckpointFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5) MapPartitionsRDD[578] at coalesce at NativeMethodAccessorImpl.java:0 []\n",
      " |  ReliableCheckpointRDD[580] at count at <ipython-input-200-87fc17c4d4eb>:1 []\n"
     ]
    }
   ],
   "source": [
    "print finalRdd.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using shared variables to communicate with Spark executors.\n",
    "\n",
    "\n",
    "- Normally, when a function passed to a Spark operation is executed, it works on separate copies of all the variables used in the function. \n",
    "- These variables are copied to each partition, and updates to the variables are not propagated back to the driver program.\n",
    "\n",
    "- Solution : Shared variables, help maintain a global state or share data across tasks and partitions.\n",
    "\n",
    "**1. Accumulator : Aggregate information from executor nodes to the driver node - Write-only**\n",
    "- Is shared across executors that you can only add to.\n",
    "- Useful to implement global sums and counters.\n",
    "- Can be accessed by **driver node**, not from an executor node.\n",
    "- Create using\n",
    "\n",
    "```python\n",
    "sc.accumulator(inital_value)\n",
    "```\n",
    "\n",
    "- The executor can add to the accumulator with .add(val) method or +. (it is **write-only.**) ==> Cannot read!\n",
    "- The driver can call it using `.value` - NOT the executor. i.e. you cannot pass it within a transformation or action\n",
    "- Not optimal for production. Most for debugging\n",
    "    \n",
    "**2. Broadcast variable : Efficiently distribute large read-only values to executor nodes - Read-only**.\n",
    "- Efficiently distribute large read-only values such as lookup tables to executor nodes.\n",
    "- The value is sent to each node only once (not per task). \n",
    "- Create a broadcast variable using \n",
    "\n",
    "```\n",
    "sc.broadcast(value)\n",
    "```\n",
    "\n",
    "- If pass it via a map/reduce function ==> Error!\n",
    "- Access the value with `.value.`\n",
    "- Should `.unpersist()` for removing a broadcast variable from memory on all workers.\n",
    "- example: `rdd.filter(lambda x: x[1] == prod.value)` is good but you can't have `prod.value+1`\n",
    "\n",
    "- Advantages\n",
    "    + Use an efficient and scalable peer-to-peer distribution mechanism. \n",
    "    + Eliminate the need for a shuffle operation.\n",
    "    + Replicate data once per worker (not once per task). \n",
    "    + Are serialized objects (can be read efficiently.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9\n",
    "- Define a accumulator variable and initialize the value to be 0.\n",
    "- Generate values between 1 and 10,000,000 using .parallelize().\n",
    "- For each value, increment accumulator variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize(range(1, 10000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums.foreach(lambda x: acc.add(1)) # for each value, increment accumulator variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.value # similar as list.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exception occurs when you tried to access accumulator values in executor nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulator.value cannot be accessed inside tasks\n"
     ]
    }
   ],
   "source": [
    "try: nums.foreach(lambda x: acc.value)\n",
    "except: print(\"Accumulator.value cannot be accessed inside tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 10\n",
    "\n",
    "- Count how many status data are collected where station_id is ’10’.\n",
    "- Compare an output of 1) .count(), 2) an accumulator value incremented within a transformation and 3) an accumulator value incremented within an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = sc.textFile('../hw1/input_1/status.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bike = raw.map(lambda x: ((x.split(\",\"))[0], x))\n",
    "bike = raw.map(lambda x: x.split(\",\")).map(lambda x: (x[0], x[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike = bike.partitionBy(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'13', [u'7', u'8', u'\"2014-09-26 08:12:02\"'])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_filtered1 = bike.filter(lambda x: x[0]=='10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.7 ms, sys: 4.67 ms, total: 13.4 ms\n",
      "Wall time: 34.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "523623"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bike_filtered1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'10', [u'7', u'8', u'\"2014-12-30 15:37:02\"'])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_filtered1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc1 = sc.accumulator(0) # within transformation\n",
    "acc2 = sc.accumulator(0) # within action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an accumulator value incremented within a transformation\n",
    "# does not always work, should work for \n",
    "def filter_st(x): \n",
    "    if (x[0]=='10'): \n",
    "        acc1.add(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bike_filtered2 = bike.map(filter_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 ms, sys: 10.7 ms, total: 20.9 ms\n",
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# an accumulator value incremented within an action.\n",
    "bike.foreach(lambda x: acc2.add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36647622\n",
      "1047246\n",
      "36647622\n"
     ]
    }
   ],
   "source": [
    "print bike_filtered2.count()\n",
    "print acc1.value\n",
    "print acc2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_filtered2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 11\n",
    "Generate a broadcast variable and access, modify and unpersist it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.broadcast.Broadcast"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize(range(10, 100))\n",
    "bc = sc.broadcast([1,2,3])\n",
    "type(bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_bc = nums.map(lambda x: x + bc.value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bc.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Broadcast' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7c1ed5e1e5e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Broadcast' and 'int'"
     ]
    }
   ],
   "source": [
    "bc = bc + 1\n",
    "bc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-c283afb9e7da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "bc.value + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.unpersist() # unpersist method removes from executor node but still stays on driver node\n",
    "bc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 12 \n",
    "\n",
    "Data\n",
    "- bike_share/stations.csv includes station_id, num_bikes_available, num_docks_available and timestamp.\n",
    "- bike_share/status.csv includes station_id, name, lat, lon, total_num_dock, city, station_installed_date.\n",
    "\n",
    "\n",
    "Write a code returns timestamp, name, num_bikes_available, num_docks_available of a given station name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 16.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file1 = \"../hw1/input_1/status_small.csv\"\n",
    "input_file2 = \"../hw1/input_1/stations.csv\"\n",
    "station_name = \"San Jose City Hall\"\n",
    "\n",
    "# # Stations\n",
    "# raw2 = sc.textFile(input_file2, 10)\n",
    "# stations = raw2.map(lambda x: x.split(\",\")).map(lambda x: (x[1], x[0]))\n",
    "# stationId_input = stations.lookup(station_name)\n",
    "\n",
    "# if len(stationId_input) == 1:\n",
    "#     stationId_input = stationId_input[0]\n",
    "# else:\n",
    "#     stationId_input = \"\"\n",
    "\n",
    "# # Status\n",
    "# raw1 = sc.textFile(input_file1, 10)\n",
    "# status = raw1.map(lambda x: x.split(\",\")).map(lambda x: (x[0], ([x[3], x[1], x[2]])))\n",
    "# status = status.partitionBy(70).persist()\n",
    "# station_details = status.lookup(stationId_input)\n",
    "\n",
    "# for e in sorted(station_details):\n",
    "#     print \",\".join(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = sc.accumulator(0)\n",
    "list = sc.parallelize(range(1,5))\n",
    "list.foreach(lambda x: acc.add(1))\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: Accumulator.value cannot be accessed inside tasks\n"
     ]
    }
   ],
   "source": [
    "try: list.map(lambda x: x + acc.value).collect()\n",
    "except: print('Exception: Accumulator.value cannot be accessed inside tasks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "list = sc.parallelize(range(1,5))\n",
    "print list.collect()\n",
    "bc = sc.broadcast([1,2,3])\n",
    "print list.map(lambda x: x + bc.value[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 13 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = './data/USF_Mission.txt'\n",
    "lines = sc.textFile(file_name)\n",
    "word = lines.flatMap(lambda l: l.split())\n",
    "word_map = word.map(lambda w: (w,1))\n",
    "word_map.collect()\n",
    "word_count = word_map.reduceByKey(lambda a,b: a+b) # shuffle\n",
    "finalRdd = word_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[91] at collect at <ipython-input-76-155699745f6c>:7 []\n",
      " |  MapPartitionsRDD[90] at mapPartitions at PythonRDD.scala:427 []\n",
      " |  ShuffledRDD[89] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[88] at reduceByKey at <ipython-input-76-155699745f6c>:6 []\n",
      "    |  PythonRDD[87] at reduceByKey at <ipython-input-76-155699745f6c>:6 []\n",
      "    |  ./data/USF_Mission.txt MapPartitionsRDD[85] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  ./data/USF_Mission.txt HadoopRDD[84] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print word_count.toDebugString()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
